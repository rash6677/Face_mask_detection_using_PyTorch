{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies related to nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#importing dependencies related to image transformations\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "#importing dependencies related to data loading\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#importing Tensorboard for data visualization\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining image transformations\n",
    "\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data = {\n",
    "    'train':\n",
    "    datasets.ImageFolder(root='./data/train', transform=image_transforms['train']),\n",
    "    'test':\n",
    "    datasets.ImageFolder(root='./data/test', transform=image_transforms['test']),\n",
    "}\n",
    "\n",
    "# Dataloader iterators, used for making batches\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=100, shuffle=True),\n",
    "    'test': DataLoader(data['test'], batch_size=100, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using cache found in C:\\Users\\Rakshit/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
    }
   ],
   "source": [
    "#loading MobileNetv2\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'mobilenet_v2', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing the initial layers of MobileNetv2\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding our own classifier\n",
    "model.classifier[1] = nn.Sequential(\n",
    "                      nn.Linear(1280, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256, 128),\n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 32),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(32, 2), \n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "328450\n"
    }
   ],
   "source": [
    "#checking trainable parameters\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training on GPU\n"
    }
   ],
   "source": [
    "#checking GPU Avaibility\n",
    "if torch.cuda.is_available():\n",
    "    print('training on GPU')\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print('training on CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "from tqdm.notebook import tqdm #for loading bars\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs,device):\n",
    "    writer = SummaryWriter(\"saves/logs\")\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in (train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in (val_loader):\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)          \n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.4f}'.format(epoch, training_loss,valid_loss, num_correct / num_examples))\n",
    "        \n",
    "        #save model\n",
    "        if(training_loss<0.005):\n",
    "            torch.save(model,\"saves/\"+str(epoch)+\".pth\")\n",
    "        \n",
    "        #tensorBoard save log\n",
    "        writer.add_scalar('Loss', loss.item(), epoch)\n",
    "        writer.add_scalar('Train/Loss', training_loss, epoch)\n",
    "        writer.add_scalar('Test/Loss', valid_loss, epoch)\n",
    "        writer.add_scalar('Test/Accuracy',num_correct / num_examples, epoch)\n",
    "        writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "def test_model(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['test']:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('correct: {:d}  total: {:d}'.format(correct, total))\n",
    "    print('accuracy = {:f}'.format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put model to GPU (if available)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing optimizer and loss Function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0), HTML(value=&#39;&#39;)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d7fd47622754ce380650b3b19c765a8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 0, Training Loss: 0.4110, Validation Loss: 0.1502, accuracy = 0.9970\nEpoch: 1, Training Loss: 0.1368, Validation Loss: 0.0555, accuracy = 0.9970\nEpoch: 2, Training Loss: 0.0925, Validation Loss: 0.0408, accuracy = 1.0000\nEpoch: 3, Training Loss: 0.0728, Validation Loss: 0.0299, accuracy = 1.0000\nEpoch: 4, Training Loss: 0.0638, Validation Loss: 0.0203, accuracy = 1.0000\nEpoch: 5, Training Loss: 0.0527, Validation Loss: 0.0153, accuracy = 1.0000\nEpoch: 6, Training Loss: 0.0498, Validation Loss: 0.0129, accuracy = 1.0000\nEpoch: 7, Training Loss: 0.0478, Validation Loss: 0.0117, accuracy = 1.0000\nEpoch: 8, Training Loss: 0.0416, Validation Loss: 0.0102, accuracy = 1.0000\nEpoch: 9, Training Loss: 0.0465, Validation Loss: 0.0104, accuracy = 1.0000\nEpoch: 10, Training Loss: 0.0384, Validation Loss: 0.0092, accuracy = 1.0000\nEpoch: 11, Training Loss: 0.0387, Validation Loss: 0.0083, accuracy = 1.0000\nEpoch: 12, Training Loss: 0.0464, Validation Loss: 0.0078, accuracy = 1.0000\nEpoch: 13, Training Loss: 0.0393, Validation Loss: 0.0091, accuracy = 1.0000\nEpoch: 14, Training Loss: 0.0377, Validation Loss: 0.0070, accuracy = 1.0000\nEpoch: 15, Training Loss: 0.0364, Validation Loss: 0.0056, accuracy = 1.0000\nEpoch: 16, Training Loss: 0.0379, Validation Loss: 0.0061, accuracy = 1.0000\nEpoch: 17, Training Loss: 0.0318, Validation Loss: 0.0082, accuracy = 1.0000\nEpoch: 18, Training Loss: 0.0395, Validation Loss: 0.0065, accuracy = 1.0000\nEpoch: 19, Training Loss: 0.0364, Validation Loss: 0.0050, accuracy = 1.0000\nEpoch: 20, Training Loss: 0.0311, Validation Loss: 0.0045, accuracy = 1.0000\nEpoch: 21, Training Loss: 0.0319, Validation Loss: 0.0040, accuracy = 1.0000\nEpoch: 22, Training Loss: 0.0326, Validation Loss: 0.0047, accuracy = 1.0000\nEpoch: 23, Training Loss: 0.0321, Validation Loss: 0.0046, accuracy = 1.0000\nEpoch: 24, Training Loss: 0.0314, Validation Loss: 0.0047, accuracy = 1.0000\nEpoch: 25, Training Loss: 0.0306, Validation Loss: 0.0046, accuracy = 1.0000\nEpoch: 26, Training Loss: 0.0268, Validation Loss: 0.0043, accuracy = 1.0000\nEpoch: 27, Training Loss: 0.0287, Validation Loss: 0.0048, accuracy = 1.0000\nEpoch: 28, Training Loss: 0.0289, Validation Loss: 0.0053, accuracy = 1.0000\nEpoch: 29, Training Loss: 0.0352, Validation Loss: 0.0048, accuracy = 1.0000\nEpoch: 30, Training Loss: 0.0257, Validation Loss: 0.0037, accuracy = 1.0000\nEpoch: 31, Training Loss: 0.0240, Validation Loss: 0.0064, accuracy = 0.9970\nEpoch: 32, Training Loss: 0.0310, Validation Loss: 0.0074, accuracy = 0.9970\nEpoch: 33, Training Loss: 0.0324, Validation Loss: 0.0041, accuracy = 1.0000\nEpoch: 34, Training Loss: 0.0290, Validation Loss: 0.0052, accuracy = 1.0000\nEpoch: 35, Training Loss: 0.0252, Validation Loss: 0.0043, accuracy = 1.0000\nEpoch: 36, Training Loss: 0.0271, Validation Loss: 0.0030, accuracy = 1.0000\nEpoch: 37, Training Loss: 0.0286, Validation Loss: 0.0045, accuracy = 1.0000\nEpoch: 38, Training Loss: 0.0297, Validation Loss: 0.0033, accuracy = 1.0000\nEpoch: 39, Training Loss: 0.0254, Validation Loss: 0.0034, accuracy = 1.0000\nEpoch: 40, Training Loss: 0.0257, Validation Loss: 0.0031, accuracy = 1.0000\nEpoch: 41, Training Loss: 0.0261, Validation Loss: 0.0027, accuracy = 1.0000\nEpoch: 42, Training Loss: 0.0281, Validation Loss: 0.0060, accuracy = 0.9970\nEpoch: 43, Training Loss: 0.0278, Validation Loss: 0.0058, accuracy = 0.9970\nEpoch: 44, Training Loss: 0.0254, Validation Loss: 0.0028, accuracy = 1.0000\nEpoch: 45, Training Loss: 0.0244, Validation Loss: 0.0030, accuracy = 1.0000\nEpoch: 46, Training Loss: 0.0312, Validation Loss: 0.0038, accuracy = 1.0000\nEpoch: 47, Training Loss: 0.0254, Validation Loss: 0.0029, accuracy = 1.0000\nEpoch: 48, Training Loss: 0.0290, Validation Loss: 0.0071, accuracy = 0.9970\nEpoch: 49, Training Loss: 0.0305, Validation Loss: 0.0027, accuracy = 1.0000\nEpoch: 50, Training Loss: 0.0292, Validation Loss: 0.0036, accuracy = 1.0000\nEpoch: 51, Training Loss: 0.0243, Validation Loss: 0.0037, accuracy = 1.0000\nEpoch: 52, Training Loss: 0.0231, Validation Loss: 0.0029, accuracy = 1.0000\nEpoch: 53, Training Loss: 0.0291, Validation Loss: 0.0037, accuracy = 1.0000\nEpoch: 54, Training Loss: 0.0257, Validation Loss: 0.0028, accuracy = 1.0000\nEpoch: 55, Training Loss: 0.0293, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 56, Training Loss: 0.0257, Validation Loss: 0.0030, accuracy = 1.0000\nEpoch: 57, Training Loss: 0.0262, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 58, Training Loss: 0.0214, Validation Loss: 0.0024, accuracy = 1.0000\nEpoch: 59, Training Loss: 0.0250, Validation Loss: 0.0026, accuracy = 1.0000\nEpoch: 60, Training Loss: 0.0248, Validation Loss: 0.0029, accuracy = 1.0000\nEpoch: 61, Training Loss: 0.0217, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 62, Training Loss: 0.0228, Validation Loss: 0.0021, accuracy = 1.0000\nEpoch: 63, Training Loss: 0.0212, Validation Loss: 0.0033, accuracy = 1.0000\nEpoch: 64, Training Loss: 0.0235, Validation Loss: 0.0044, accuracy = 0.9970\nEpoch: 65, Training Loss: 0.0238, Validation Loss: 0.0054, accuracy = 0.9970\nEpoch: 66, Training Loss: 0.0235, Validation Loss: 0.0027, accuracy = 1.0000\nEpoch: 67, Training Loss: 0.0263, Validation Loss: 0.0052, accuracy = 0.9970\nEpoch: 68, Training Loss: 0.0270, Validation Loss: 0.0032, accuracy = 1.0000\nEpoch: 69, Training Loss: 0.0240, Validation Loss: 0.0033, accuracy = 1.0000\nEpoch: 70, Training Loss: 0.0231, Validation Loss: 0.0031, accuracy = 1.0000\nEpoch: 71, Training Loss: 0.0253, Validation Loss: 0.0028, accuracy = 1.0000\nEpoch: 72, Training Loss: 0.0226, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 73, Training Loss: 0.0218, Validation Loss: 0.0034, accuracy = 1.0000\nEpoch: 74, Training Loss: 0.0258, Validation Loss: 0.0039, accuracy = 1.0000\nEpoch: 75, Training Loss: 0.0252, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 76, Training Loss: 0.0244, Validation Loss: 0.0036, accuracy = 1.0000\nEpoch: 77, Training Loss: 0.0201, Validation Loss: 0.0026, accuracy = 1.0000\nEpoch: 78, Training Loss: 0.0207, Validation Loss: 0.0023, accuracy = 1.0000\nEpoch: 79, Training Loss: 0.0221, Validation Loss: 0.0026, accuracy = 1.0000\nEpoch: 80, Training Loss: 0.0273, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 81, Training Loss: 0.0217, Validation Loss: 0.0056, accuracy = 0.9970\nEpoch: 82, Training Loss: 0.0203, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 83, Training Loss: 0.0246, Validation Loss: 0.0028, accuracy = 1.0000\nEpoch: 84, Training Loss: 0.0211, Validation Loss: 0.0032, accuracy = 1.0000\nEpoch: 85, Training Loss: 0.0197, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 86, Training Loss: 0.0225, Validation Loss: 0.0057, accuracy = 0.9970\nEpoch: 87, Training Loss: 0.0220, Validation Loss: 0.0024, accuracy = 1.0000\nEpoch: 88, Training Loss: 0.0185, Validation Loss: 0.0029, accuracy = 1.0000\nEpoch: 89, Training Loss: 0.0157, Validation Loss: 0.0025, accuracy = 1.0000\nEpoch: 90, Training Loss: 0.0214, Validation Loss: 0.0036, accuracy = 0.9970\nEpoch: 91, Training Loss: 0.0212, Validation Loss: 0.0021, accuracy = 1.0000\nEpoch: 92, Training Loss: 0.0251, Validation Loss: 0.0022, accuracy = 1.0000\nEpoch: 93, Training Loss: 0.0222, Validation Loss: 0.0023, accuracy = 1.0000\nEpoch: 94, Training Loss: 0.0207, Validation Loss: 0.0057, accuracy = 0.9970\nEpoch: 95, Training Loss: 0.0198, Validation Loss: 0.0065, accuracy = 0.9970\nEpoch: 96, Training Loss: 0.0217, Validation Loss: 0.0020, accuracy = 1.0000\nEpoch: 97, Training Loss: 0.0215, Validation Loss: 0.0023, accuracy = 1.0000\nEpoch: 98, Training Loss: 0.0158, Validation Loss: 0.0018, accuracy = 1.0000\nEpoch: 99, Training Loss: 0.0265, Validation Loss: 0.0018, accuracy = 1.0000\n\n"
    }
   ],
   "source": [
    "#train the model first 100 epochs\n",
    "train(model, optimizer,loss,dataloaders['train'],dataloaders['test'],100,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "correct: 330  total: 330\naccuracy = 1.000000\n"
    }
   ],
   "source": [
    "#test the model\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(model, \"./firstHundread.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}